{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c4b343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/CS3244_ML_Project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from functions import data_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe6d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data_pipeline(model_name='XGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59ad744",
   "metadata": {},
   "source": [
    "## Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500f9841",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c5ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_components(X_train, method = \"avg\"):\n",
    "        pca = PCA()\n",
    "        pca.fit(X_train)\n",
    "        explained_var_ratio = pca.explained_variance_ratio_\n",
    "        if method == \"avg\":\n",
    "            avg_var = 1 / len(explained_var_ratio)\n",
    "            optimal_components = np.sum(explained_var_ratio > avg_var)\n",
    "\n",
    "        elif method == \"elbow\":\n",
    "            diffs = np.diff(explained_var_ratio)\n",
    "            elbow_idx = np.argmax(diffs * -1) + 1 \n",
    "            optimal_components = elbow_idx\n",
    "\n",
    "        elif method == \"cumulative\":\n",
    "            cum_var = np.cumsum(explained_var_ratio)\n",
    "            optimal_components = np.argmax(cum_var >= 0.95) + 1\n",
    "        else:\n",
    "            # comparison with a base model to see if PCA methods are actually improving the model\n",
    "            optimal_components = None\n",
    "        return optimal_components\n",
    "    \n",
    "methods = [\"avg\", \"elbow\", \"cumulative\", \"default\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0449af",
   "metadata": {},
   "source": [
    "### Transformations & Scaling\n",
    "- from QQ plot analysis - noticed that certain numerical features require transformation and diff features need different scalers\n",
    "> Highly skewed & outlier-heavy\trisk_score, months_employed\t→ YeoJohnsonTransformer() or np.log1p() → RobustScaler\n",
    "\n",
    "> Already normal/log-transformed\tamt_income_total_log, age\t→ StandardScaler\n",
    "\n",
    "> Discrete / ordinal numeric\tcnt_children, cnt_fam_members\t→ Keep as is or encode as ordinal integers\n",
    "\n",
    "### Encoding\n",
    "* categorical features - label encoder might assign encoded categories some inherent ordering affecting model which is fine for tree based models & XGBoost but not for SVC and KNN, so need to use diff encoding methods that suit the diff models\n",
    "\n",
    "| Feature type                  | XGBoost                     | SVC          | KNN                       |\n",
    "| ----------------------------- | --------------------------- | ------------ | ------------------------- |\n",
    "| Binary                        | 0/1 mapping                 | 0/1 mapping  | 0/1 mapping               |\n",
    "| Low-cardinality (<5)          | One-hot or label encoding   | One-hot only | One-hot                   |\n",
    "| Medium/high-cardinality (~17) | Frequency or label encoding | One-hot only | One-hot / binary encoding |\n",
    "| Numeric                       | Raw                         | Standardized | Standardized              |\n",
    "\n",
    "## Dropping of correlated features\n",
    "| Feature type     | XGBoost / Tree                      | SVC / KNN / Linear                |\n",
    "| ---------------- | ----------------------------------- | --------------------------------- |\n",
    "| Discrete numeric | keep numeric                        | Better as categorical / one-hot   |\n",
    "| Binned/ordinal   | Optional (tree can handle either)   | Use one-hot encoding              |\n",
    "\n",
    "## Feature Selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c4dfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import (\n",
    "    PowerTransformer, RobustScaler, StandardScaler,\n",
    "    OneHotEncoder, OrdinalEncoder, FunctionTransformer\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, RFE, SelectFromModel\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Column Dictionary\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "numeric_cols = [\"age\", \"cnt_children\", \"amt_income_total_log\", \"risk_score\", \"months_employed\"]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Build Transformer\n",
    "# ------------------------------------------------------------\n",
    "def build_transformer():\n",
    "    \n",
    "    transformers=[\n",
    "            (\"num\", StandardScaler(), numeric_cols)\n",
    "        ]\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers, remainder='passthrough')\n",
    "    \n",
    "    return preprocessor\n",
    "# ------------------------------------------------------------\n",
    "# 2. Drop Correlated Features\n",
    "# ------------------------------------------------------------\n",
    "# def drop_correlated_features(model_name, col_dic=column_dic):\n",
    "drop_cols = [\"days_birth\", \"amt_income_total\", \"years_employed\", \"flag_mobil\", \"code_gender\", \"flag_own_realty\", \"flag_own_car\", \"cnt_fam_members\"]\n",
    "    # if model_name in [\"SVC\", \"KNN\"]:\n",
    "    #     drop_cols.extend([\"cnt_children\", \"cnt_fam_members\"])\n",
    "    #     drop_cols.extend\n",
    "    # return drop_cols\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Feature Selection Model\n",
    "# ------------------------------------------------------------\n",
    "def build_feature_selector(model_name):\n",
    "    if model_name==\"SVC\":\n",
    "        return RFE(SVC(kernel='linear'), n_features_to_select=None, step=0.2, importance_getter='feature_importances_')\n",
    "    elif model_name==\"XGB\":\n",
    "        return SelectFromModel(XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42), threshold='median')\n",
    "    elif model_name==\"KNN\":\n",
    "        return SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "# ------------------------------------------------------------\n",
    "# 4. Build Model\n",
    "# ------------------------------------------------------------\n",
    "def build_model(model_name):\n",
    "    if model_name == \"SVC\":\n",
    "        return \"SVM (Linear)\", SVC(kernel='linear', random_state=42)\n",
    "    elif model_name == \"XGB\":\n",
    "        return \"XGB Classifier\", XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "    elif model_name == \"KNN\":\n",
    "        return \"KNN\", KNeighborsClassifier()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model name\")\n",
    "    \n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Model Training Pipeline\n",
    "# ------------------------------------------------------------\n",
    "def model_pipeline(model_name, train_df, test_df, target_col=\"label\", random_state=42):\n",
    "    # Drop correlated columns\n",
    "    drop_cols = [\"days_birth\", \"amt_income_total\", \"years_employed\", \"flag_mobil\", \"code_gender\", \"flag_own_realty\", \"flag_own_car\", \"cnt_fam_members\"]\n",
    "    train_df = train_df.drop(columns=drop_cols, errors='ignore')\n",
    "    test_df = test_df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "    # Split features and target\n",
    "    X_train_full = train_df.drop(columns=[target_col])\n",
    "    y_train_full = train_df[target_col]\n",
    "    X_test = test_df.drop(columns=[target_col])\n",
    "    y_test = test_df[target_col]\n",
    "\n",
    "    # Preprocessor\n",
    "    # preprocessor = build_transformer()\n",
    "    \n",
    "    # Model\n",
    "    name, model = build_model(model_name)\n",
    "    print(f\"\\nTraining model: {name} using StratifiedKFold...\")\n",
    "\n",
    "    # Stratified K-Fold\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    acc_scores, f1_scores, roc_scores = [], [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_full, y_train_full), 1):\n",
    "        X_train, X_val = X_train_full.iloc[train_idx], X_train_full.iloc[val_idx]\n",
    "        y_train, y_val = y_train_full.iloc[train_idx], y_train_full.iloc[val_idx]\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            # (\"preprocess\", preprocessor),\n",
    "            (\"feature_selector\", build_feature_selector(model_name)),\n",
    "            (\"classifier\", model)\n",
    "        ])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        y_proba = pipeline.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "\n",
    "        acc_scores.append(accuracy_score(y_val, y_pred))\n",
    "        f1_scores.append(f1_score(y_val, y_pred))\n",
    "        roc_scores.append(roc_auc_score(y_val, y_proba))\n",
    "\n",
    "        print(f\"Fold {fold}: Accuracy={acc_scores[-1]:.3f}, F1={f1_scores[-1]:.3f}, ROC-AUC={roc_scores[-1]:.3f}\")\n",
    "\n",
    "    results = {\n",
    "        \"model\": name,\n",
    "        \"accuracy\": np.mean(acc_scores),\n",
    "        \"f1_score\": np.mean(f1_scores),\n",
    "        \"roc_auc\": np.nanmean(roc_scores)\n",
    "    }\n",
    "\n",
    "    print(f\"\\nFinished training {name} across all folds.\")\n",
    "    print(f\"Average Accuracy: {results['accuracy']:.3f}, F1: {results['f1_score']:.3f}, ROC-AUC: {results['roc_auc']:.3f}\")\n",
    "\n",
    "    return results, X_train_full, y_train_full, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a943aeee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n",
      "\u001b[32m      1\u001b[39m models = [\u001b[33m\"\u001b[39m\u001b[33mXGB\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mKNN\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n",
      "\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# train_df, test_df = data_pipeline()\u001b[39;00m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     results_df, X_train, y_train, X_test, y_test = \u001b[43mmodel_pipeline\u001b[49m(model_name=model, \n",
      "\u001b[32m      5\u001b[39m         train_df=train, test_df=test, target_col=\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m, random_state=\u001b[32m42\u001b[39m\n",
      "\u001b[32m      6\u001b[39m     )\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'model_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "models = [\"XGB\", \"KNN\"]\n",
    "for model in models:\n",
    "    # train_df, test_df = data_pipeline()\n",
    "    results_df, X_train, y_train, X_test, y_test = model_pipeline(model_name=model, \n",
    "        train_df=train, test_df=test, target_col=\"label\", random_state=42\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
