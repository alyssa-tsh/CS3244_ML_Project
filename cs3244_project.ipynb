{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alyssa-tsh/CS3244_ML_Project/blob/main/cs3244_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1U29HV8DBvCi"
      },
      "outputs": [],
      "source": [
        "from functions import data_pipeline, data_pipeline_svc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data\n",
            "Splitting credits data\n",
            "Total unique accounts: 45985. Starting to find cutoff point\n",
            "Cutoff month where CDF reaches 80%: -10\n",
            "\n",
            "=== Split based on CDF 80% cutoff ===\n",
            "Cutoff month: -10 (10 months ago)\n",
            "Old accounts (≤ month -10): 37,210 (80.9%)\n",
            "New accounts (> month -10): 8,775 (19.1%)\n",
            "Ratio (old/new): 4.2405\n",
            "Splitting raw credit records\n",
            "Cleaning old accounts credit records - [Length: 996586]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/workspaces/CS3244_ML_Project/functions.py:154: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  final_df = df.groupby(['id', 'origination_month']).apply(lambda x: pd.Series({\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning new accounts credit records - [Length: 51989]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/workspaces/CS3244_ML_Project/functions.py:154: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  final_df = df.groupby(['id', 'origination_month']).apply(lambda x: pd.Series({\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning credit data completed\n",
            "Splitting applications data\n",
            "Splitting application dataset\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'id'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/CS3244_ML_Project/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mKeyError\u001b[39m: 'id'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train, test = \u001b[43mdata_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# train_svc, test_svc = data_pipeline_svc()\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/CS3244_ML_Project/functions.py:340\u001b[39m, in \u001b[36mdata_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mCleaning credit data completed\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    339\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mSplitting applications data\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m old_accounts_application_df, new_accounts_application_df = \u001b[43msplit_application_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapplication_records_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mCleaning old application records\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    342\u001b[39m old_accounts_application = clean_application_records(old_accounts_application_df)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/CS3244_ML_Project/functions.py:224\u001b[39m, in \u001b[36msplit_application_dataset\u001b[39m\u001b[34m(df, old_ids, new_ids)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msplit_application_dataset\u001b[39m(df, old_ids, new_ids):\n\u001b[32m    222\u001b[39m   \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mSplitting application dataset\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m   dupes_df = df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.duplicated(keep=\u001b[38;5;28;01mFalse\u001b[39;00m)].sort_values(by=\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    225\u001b[39m   dupes_df = drop_id_dupes(dupes_df)\n\u001b[32m    226\u001b[39m   non_dupes = df[~df[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m].duplicated(keep=\u001b[38;5;28;01mFalse\u001b[39;00m)]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/CS3244_ML_Project/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/CS3244_ML_Project/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
            "\u001b[31mKeyError\u001b[39m: 'id'"
          ]
        }
      ],
      "source": [
        "train, test = data_pipeline()\n",
        "# train_svc, test_svc = data_pipeline_svc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, OneHotEncoder, FunctionTransformer, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer \n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_n_components(X_train, method = \"avg\"):\n",
        "        pca = PCA()\n",
        "        pca.fit(X_train)\n",
        "        explained_var_ratio = pca.explained_variance_ratio_\n",
        "        if method == \"avg\":\n",
        "            avg_var = 1 / len(explained_var_ratio)\n",
        "            optimal_components = np.sum(explained_var_ratio > avg_var)\n",
        "\n",
        "        elif method == \"elbow\":\n",
        "            diffs = np.diff(explained_var_ratio)\n",
        "            elbow_idx = np.argmax(diffs * -1) + 1 \n",
        "            optimal_components = elbow_idx\n",
        "\n",
        "        elif method == \"cumulative\":\n",
        "            cum_var = np.cumsum(explained_var_ratio)\n",
        "            optimal_components = np.argmax(cum_var >= 0.95) + 1\n",
        "        else:\n",
        "            # comparison with a base model to see if PCA methods are actually improving the model\n",
        "            optimal_components = None\n",
        "        return optimal_components\n",
        "    \n",
        "methods = [\"avg\", \"elbow\", \"cumulative\", \"default\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformations & Scaling\n",
        "- from QQ plot analysis - noticed that certain numerical features require transformation and diff features need different scalers\n",
        "> Highly skewed & outlier-heavy\trisk_score, months_employed\t→ YeoJohnsonTransformer() or np.log1p() → RobustScaler\n",
        "\n",
        "> Already normal/log-transformed\tamt_income_total_log, age\t→ StandardScaler\n",
        "\n",
        "> Discrete / ordinal numeric\tcnt_children, cnt_fam_members\t→ Keep as is or encode as ordinal integers\n",
        "\n",
        "### Encoding\n",
        "* REALIZED that there are a lot of categorical features - label encoder might assign encoded categories some inherent ordering affecting model which is fine for tree based models & XGBoost but not for SVC and KNN\n",
        "\n",
        "| Feature type                  | XGBoost                     | SVC          | KNN                       |\n",
        "| ----------------------------- | --------------------------- | ------------ | ------------------------- |\n",
        "| Binary                        | 0/1 mapping                 | 0/1 mapping  | 0/1 mapping               |\n",
        "| Low-cardinality (<5)          | One-hot or label encoding   | One-hot only | One-hot                   |\n",
        "| Medium/high-cardinality (~17) | Frequency or label encoding | One-hot only | One-hot / binary encoding |\n",
        "| Numeric                       | Raw                         | Standardized | Standardized              |\n",
        "\n",
        "## Dropping of correlated features\n",
        "| Feature type     | XGBoost / Tree                      | SVC / KNN / Linear                |\n",
        "| ---------------- | ----------------------------------- | --------------------------------- |\n",
        "| Discrete numeric | keep numeric                        | Better as categorical / one-hot   |\n",
        "| Binned/ordinal   | Optional (tree can handle either)   | Use one-hot encoding              |\n",
        "\n",
        "## Feature Selection\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "column_dic = {\n",
        "     # assigning numeric_cols specific scalers and transformations based on QQ plot analysis\n",
        "    \"skewed\" : ['risk_score', 'months_employed'],\n",
        "    \"normal\" : ['amt_income_total_log', 'age'],\n",
        "    # \"discrete\" : ['cnt_children', 'cnt_fam_members'],\n",
        "    # categorical variables\n",
        "    \"low_card_cols\" : [\"name_income_type\",\"name_education_type\",\"name_family_status\",\"name_housing_type\"],\n",
        "    \"high_card_cols\" : [\"occupation_type\", \"aged_binned\"]\n",
        "}\n",
        "def build_transformer(model_name, column_dic):\n",
        "    transformer = []\n",
        "    transformer.append(\n",
        "        [\n",
        "            ('skewed', Pipeline([\n",
        "            ('yeo', PowerTransformer(method='yeo-johnson')),\n",
        "            ('robust', RobustScaler())\n",
        "            ]), column_dic[\"skewed\"]), \n",
        "            ('normal', StandardScaler(), column_dic[\"normal\"])                                                                 \n",
        "        ])\n",
        "    \n",
        "    if model_name == \"SVC\" or model_name == \"KNN\":\n",
        "        print(\"Building transformer for model:\", model_name)\n",
        "        transformer.append(\n",
        "            [   \n",
        "                (\"cat\", OneHotEncoder(handle_unknown='ignore'), column_dic[\"low_card_cols\"] + column_dic[\"high_card_cols\"])\n",
        "            ]\n",
        "        )\n",
        "    elif model_name == \"XGB\":\n",
        "        print(\"Building transformer for model:\", model_name)\n",
        "        transformer.append(\n",
        "            [   \n",
        "                (\"low_cat\", OneHotEncoder(handle_unknown='ignore'), column_dic[\"low_card_cols\"]),\n",
        "                (\"high_cat\", FunctionTransformer(lambda X: X.assign(**{col: X[col].map(X[col].value_counts(normalize=True)) \n",
        "                                                                    for col in column_dic[\"high_card_cols\"]})), column_dic[\"high_card_cols\"])\n",
        "            ]\n",
        "        )\n",
        "    else:\n",
        "        print(\"Building default transformer model\")\n",
        "        transformer.append(\n",
        "            [(\"cat\",LabelEncoder(handle_unknown='ignore'), column_dic[\"low_card_cols\"] + column_dic[\"high_card_cols\"])]\n",
        "        )\n",
        "    return transformer\n",
        "\n",
        "def drop_correlated_features(model_name):\n",
        "    # drop highly correlated features - keeping months employed \n",
        "    drop_cols = [\"days_birth\", \"amt_income_total\", \"years_employed\", \"flag_mobil\"]\n",
        "    if model_name == \"SVC\" or model_name == \"KNN\":\n",
        "        drop_cols.append([\"cnt_children\", \"cnt_fam_members\"])\n",
        "\n",
        "    return drop_cols\n",
        "\n",
        "def build_model(model_name):\n",
        "    if model_name == \"SVC\":\n",
        "        model = {\"SVM (Linear)\" : SVC(kernel='linear', random_state=42) }       \n",
        "        return model\n",
        "    elif model_name == \"XGB\":\n",
        "        model = {\"XGB Classifier\" :  XGBClassifier(use_label_encoder=True, eval_metric=\"logloss\", random_state=42)}\n",
        "        return XGBClassifier(use_label_encoder=True, eval_metric=\"logloss\", random_state=42)\n",
        "    elif model_name == \"KNN\":\n",
        "        return KNeighborsClassifier()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported model name\")\n",
        "        \n",
        "\n",
        "def model_pipeline(model_name, train_df, test_df, target_col=\"label\", random_state=42):\n",
        "\n",
        "\n",
        "    train_df = train_df.drop(columns=drop_correlated_features(model_name))\n",
        "    test_df = test_df.drop(columns=drop_correlated_features(model_name))\n",
        "\n",
        "    train = train_df.copy()\n",
        "    test = test_df.copy()\n",
        "\n",
        "    # train test split\n",
        "    X_train_full = train.drop(columns=[target_col])\n",
        "    y_train_full = train[target_col]\n",
        "    X_test = test.drop(columns=[target_col])\n",
        "    y_test = test[target_col]\n",
        "\n",
        "    # Using RobustScaler instead since from EDA Standard & Min-Max Scaler distorted by outliers\n",
        "    preprocessor = ColumnTransformer(\n",
        "        build_transformer(model_name, column_dic)\n",
        "    )\n",
        "\n",
        "    # Define models\n",
        "    model = build_model(model_name)\n",
        "    \n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
        "    results = []\n",
        "    name = model.keys()[0]\n",
        "    print(f\"Training model: {name} with StratifiedKFold...\")\n",
        "    acc_scores, f1_scores, roc_scores = [], [], []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_full, y_train_full), 1):\n",
        "        X_train, X_val = X_train_full.iloc[train_idx], X_train_full.iloc[val_idx]\n",
        "        y_train, y_val = y_train_full.iloc[train_idx], y_train_full.iloc[val_idx]\n",
        "\n",
        "        pipeline = Pipeline([\n",
        "            (\"preprocess\", preprocessor),\n",
        "            (\"classifier\", model)\n",
        "        ])\n",
        "        pipeline.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = pipeline.predict(X_val)\n",
        "        y_proba = pipeline.predict_proba(X_val)[:,1] if hasattr(pipeline, \"predict_proba\") else None\n",
        "\n",
        "        acc_scores.append(accuracy_score(y_val, y_pred))\n",
        "        f1_scores.append(f1_score(y_val, y_pred))\n",
        "        roc_scores.append(roc_auc_score(y_val, y_proba) if y_proba is not None else np.nan)\n",
        "\n",
        "        print(f\"Fold {fold}: Accuracy={acc_scores[-1]:.3f}, F1={f1_scores[-1]:.3f}, ROC-AUC={roc_scores[-1]:.3f}\")\n",
        "\n",
        "    results.append({\n",
        "        \"model\": name,\n",
        "        \"accuracy\": np.mean(acc_scores),\n",
        "        \"f1_score\": np.mean(f1_scores),\n",
        "        \"roc_auc\": np.nanmean(roc_scores)\n",
        "    })\n",
        "    print(f\"Finished training {name} across all folds.\\n\")\n",
        "\n",
        "\n",
        "    return results, X_train_full, y_train_full, X_test, y_test\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = [\"SVC\", \"XGB\", \"KNN\"]\n",
        "for model in models:\n",
        "    train_df, test_df = data_pipeline()\n",
        "    results_df, X_train, y_train, X_test, y_test = model_pipeline(model_name=model, \n",
        "        train=train_df, test=test_df, target_col=\"label\", random_state=42\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPVW1WKkh1bEXudRF2QGP07",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
